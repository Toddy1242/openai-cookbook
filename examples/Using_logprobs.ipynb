{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using logprobs for classification and retrieval evaluation\n",
    "\n",
    "This notebook illustrates two potential uses of the `logprobs` parameter in the Chat Completions API. \n",
    "With the `logprobs` set to true, Chat Completions returns the log probabilities of each output token, and a limited number of the most likely tokens at each token position (along with their log probabilities). This can help with assessing the confidence of the model in its output, or to examine alternative responses the model may have given. While there are a wide array of use cases for logprobs, we are focusing on:<br>\n",
    "1. Classification tasks\n",
    "2. Retrieval (Q&A) evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from math import exp\n",
    "import numpy as np\n",
    "client= OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(\n",
    "    messages: list[dict[str, str]],\n",
    "    model: str = \"gpt-4\",\n",
    "    max_tokens=500,\n",
    "    temperature=1.0,\n",
    "    stop=None,\n",
    "    functions=None,\n",
    "    logprobs=None,\n",
    "    top_logprobs=None\n",
    ") -> str:\n",
    "    params = {\n",
    "        'model': model,\n",
    "        'messages': messages,\n",
    "        'max_tokens': max_tokens,\n",
    "        'temperature': temperature,\n",
    "        'stop': stop,\n",
    "        'logprobs': logprobs,\n",
    "        'top_logprobs':top_logprobs\n",
    "    }\n",
    "    if functions:\n",
    "        params['functions'] = functions\n",
    "\n",
    "    completion = client.chat.completions.create(**params)\n",
    "    return completion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to create a system to classify news articles into a set of categories. Without `logprobs`, we can use Chat Completions to do this, but it is much more difficult to assess how confident the model is in its classifications. <br><br>\n",
    "Now, with `logprobs` enabled, we can see just how confident the model is in its predictions, which is crucial for creating an accurate and trustworthy classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin with a prompt that gives the model four categories: **Technology, Politics, Sports, and Arts**, and asks the model to classify articles into those categories based on headlines alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION_PROMPT = \"\"\"You will be given a headline of a news article. Classify the article into one of the following categories: Technology, Politics, Sports, and Art.\n",
    "Return only the name of the category, and nothing else. MAKE SURE your output is one of the four categories stated. Article headline: {headline}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at three sample headlines, and first begin with a standard Chat Completions output, without `logprobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\"Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\",\n",
    "             \"Local Mayor Launches Initiative to Enhance Urban Public Transport.\",\n",
    "\"Tennis Champions Showcase Hidden Talents in Symphony Orchestra Debut\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technology \n",
      "\n",
      "Local Mayor Launches Initiative to Enhance Urban Public Transport.\n",
      "Politics \n",
      "\n",
      "Tennis Champions Showcase Hidden Talents in Symphony Orchestra Debut\n",
      "Art \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in headlines:\n",
    "  print(headline)\n",
    "  API_RESPONSE = get_completion([{'role':'user','content':CLASSIFICATION_PROMPT.format(headline=headline)}],model='gpt-4')\n",
    "  print(API_RESPONSE.choices[0].message.content,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the selected category for each headline. However, we don't know *how* confident the model is in these headlines. Let's rerun the same prompt but with `logprobs` enabled, and `top_logprobs` set to 2 (this will show us the 2 most likely output tokens). Additionally we can also output the linear probability of each output token, in order to convert the log probability to the more easily interprable scale of 0-100%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech Giant Unveils Latest Smartphone Model with Advanced Photo-Editing Features.\n",
      "\u001b[96mToken:\u001b[0m Technology, \u001b[93mlogprobs:\u001b[0m -3.1737043e-06, \u001b[95mlinear probability:\u001b[0m 100.0%\n",
      "\u001b[96mToken:\u001b[0m Techn, \u001b[93mlogprobs:\u001b[0m -13.437503, \u001b[95mlinear probability:\u001b[0m 0.0%\n",
      "\n",
      "\n",
      "Local Mayor Launches Initiative to Enhance Urban Public Transport.\n",
      "\u001b[96mToken:\u001b[0m Politics, \u001b[93mlogprobs:\u001b[0m -3.7697225e-06, \u001b[95mlinear probability:\u001b[0m 100.0%\n",
      "\u001b[96mToken:\u001b[0m Technology, \u001b[93mlogprobs:\u001b[0m -13.390629, \u001b[95mlinear probability:\u001b[0m 0.0%\n",
      "\n",
      "\n",
      "Tennis Champions Showcase Hidden Talents in Symphony Orchestra Debut\n",
      "\u001b[96mToken:\u001b[0m Sports, \u001b[93mlogprobs:\u001b[0m -0.4510038, \u001b[95mlinear probability:\u001b[0m 63.7%\n",
      "\u001b[96mToken:\u001b[0m Art, \u001b[93mlogprobs:\u001b[0m -1.0135038, \u001b[95mlinear probability:\u001b[0m 36.29%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for headline in headlines:\n",
    "  print(headline)\n",
    "  API_RESPONSE = get_completion([{'role':'user','content':CLASSIFICATION_PROMPT.format(headline=headline)}],model='gpt-4',logprobs=True, top_logprobs=2)\n",
    "  for logprob in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:\n",
    "        print(f\"\\033[96mToken:\\033[0m {logprob.token}, \\033[93mlogprobs:\\033[0m {logprob.logprob}, \\033[95mlinear probability:\\033[0m {np.round(np.exp(logprob.logprob)*100,2)}%\")\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from the first two headlines, `gpt-4` is nearly 100% confident in its classifications, as the content is clearly technology and politics focused respectively. However, the third headline combines both sports and art-related themes, so we see the model is significantly less confident in its selection, with a ~25% chance of choosing Sports instead of Art. <br><br> \n",
    "This shows how important using `logprobs` can be, as if we are using llms for classification tasks we can set confidence theshholds, or output several potential output tokens if the log probability of the selected output is not sufficiently high. For instance, if we are creating a recommendation engine to tag articles, we can automatically classify headlines crossing a certain threshold, and send the less certain headlines for manual review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Retrieval confidence scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce hallucinations, and the performance of our Q&A RAG system, we can use `logprobs` to evaluate how confident the model is in its retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have built a retrieval system using RAG for Q&A, but are struggling with hallucinated answers to our questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Article retrieved\n",
    "\n",
    "ada_lovelace_article = \"\"\"Augusta Ada King, Countess of Lovelace (née Byron; 10 December 1815 – 27 November 1852) was an English mathematician and writer, chiefly known for her work on Charles Babbage's proposed mechanical general-purpose computer, the Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation.\n",
    "Ada Byron was the only legitimate child of poet Lord Byron and reformer Lady Byron. All Lovelace's half-siblings, Lord Byron's other children, were born out of wedlock to other women. Byron separated from his wife a month after Ada was born and left England forever. He died in Greece when Ada was eight. Her mother was anxious about her upbringing and promoted Ada's interest in mathematics and logic in an effort to prevent her from developing her father's perceived insanity. Despite this, Ada remained interested in him, naming her two sons Byron and Gordon. Upon her death, she was buried next to him at her request. Although often ill in her childhood, Ada pursued her studies assiduously. She married William King in 1835. King was made Earl of Lovelace in 1838, Ada thereby becoming Countess of Lovelace.\n",
    "Her educational and social exploits brought her into contact with scientists such as Andrew Crosse, Charles Babbage, Sir David Brewster, Charles Wheatstone, Michael Faraday, and the author Charles Dickens, contacts which she used to further her education. Ada described her approach as \"poetical science\" and herself as an \"Analyst (& Metaphysician)\".\n",
    "When she was eighteen, her mathematical talents led her to a long working relationship and friendship with fellow British mathematician Charles Babbage, who is known as \"the father of computers\". She was in particular interested in Babbage's work on the Analytical Engine. Lovelace first met him in June 1833, through their mutual friend, and her private tutor, Mary Somerville.\n",
    "Between 1842 and 1843, Ada translated an article by the military engineer Luigi Menabrea (later Prime Minister of Italy) about the Analytical Engine, supplementing it with an elaborate set of seven notes, simply called \"Notes\".\n",
    "Lovelace's notes are important in the early history of computers, especially since the seventh one contained what many consider to be the first computer program—that is, an algorithm designed to be carried out by a machine. Other historians reject this perspective and point out that Babbage's personal notes from the years 1836/1837 contain the first programs for the engine. She also developed a vision of the capability of computers to go beyond mere calculating or number-crunching, while many others, including Babbage himself, focused only on those capabilities. Her mindset of \"poetical science\" led her to ask questions about the Analytical Engine (as shown in her notes) examining how individuals and society relate to technology as a collaborative tool.\n",
    "\"\"\"\n",
    "\n",
    "#Questions that can be easily answered given the article\n",
    "easy_questions = [\"What nationality was Ada Lovelace?\", \"What was an important finding from Lovelace's seventh note?\"]\n",
    "medium_questions =[\"Did Lovelace collaborate with Charles Dickens\",\"What concepts did Lovelace build with Charles Babbage\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what we can do is ask the model to respond to the question, but then also evaluate its response. Specifically, we will ask the model to output a boolean 'sufficient_context_for_answer'. We can then evaluate the `logprobs` to see just how confident the model is that its answer was contained in the provided context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"You retrieved this article: {article}. The question is: {question}. Before even answering the question, consider whether you have sufficent information in the article to answer the question fully.\n",
    "Your output should JUST be the boolean true or false, of if you have sufficient information in the article to answer the question.\n",
    "Respond with just one word, the boolean true or false.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Art'"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_RESPONSE.choices[0].logprobs.content[0].token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuestions clearly answered in article\u001b[0m\n",
      "\n",
      "\u001b[92mQuestion:\u001b[0m What nationality was Ada Lovelace?\n",
      "\u001b[96msufficient_context_for_answer:\u001b[0m True, \u001b[93mlogprobs:\u001b[0m -3.1281633e-07, \u001b[95mlinear probability:\u001b[0m 100.0% \n",
      "\n",
      "\u001b[92mQuestion:\u001b[0m What was an important finding from Lovelace's seventh note?\n",
      "\u001b[96msufficient_context_for_answer:\u001b[0m True, \u001b[93mlogprobs:\u001b[0m -3.1281633e-07, \u001b[95mlinear probability:\u001b[0m 100.0% \n",
      "\n",
      "\n",
      "\n",
      "\u001b[1mQuestions with potentially insufficient information\u001b[0m\n",
      "\n",
      "\u001b[92mQuestion:\u001b[0m Did Lovelace collaborate with Charles Dickens\n",
      "ChatCompletion(id='chatcmpl-8XJG9yst1pZoZL7M9wBUwIy6YoYuo', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='True', bytes=[84, 114, 117, 101], logprob=-0.77434313, top_logprobs=[TopLogprob(token='False', bytes=[70, 97, 108, 115, 101], logprob=-0.61809313), TopLogprob(token='True', bytes=[84, 114, 117, 101], logprob=-0.77434313), TopLogprob(token='false', bytes=[102, 97, 108, 115, 101], logprob=-11.0087185)])]), message=ChatCompletionMessage(content='True', role='assistant', function_call=None, tool_calls=None))], created=1702949549, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=683, total_tokens=684))\n",
      "\u001b[96msufficient_context_for_answer:\u001b[0m True, \u001b[93mlogprobs:\u001b[0m -0.77434313, \u001b[95mlinear probability:\u001b[0m 46.1% \n",
      "\n",
      "\u001b[92mQuestion:\u001b[0m What concepts did Lovelace build with Charles Babbage\n",
      "ChatCompletion(id='chatcmpl-8XJGAh0mjrkWwByJ0vI39nAtcXGtw', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='True', bytes=[84, 114, 117, 101], logprob=-0.52316433, top_logprobs=[TopLogprob(token='True', bytes=[84, 114, 117, 101], logprob=-0.52316433), TopLogprob(token='False', bytes=[70, 97, 108, 115, 101], logprob=-0.89816433), TopLogprob(token='false', bytes=[102, 97, 108, 115, 101], logprob=-10.460665)])]), message=ChatCompletionMessage(content='True', role='assistant', function_call=None, tool_calls=None))], created=1702949550, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=1, prompt_tokens=686, total_tokens=687))\n",
      "\u001b[96msufficient_context_for_answer:\u001b[0m True, \u001b[93mlogprobs:\u001b[0m -0.52316433, \u001b[95mlinear probability:\u001b[0m 59.26% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print('\\033[1mQuestions clearly answered in article\\033[0m\\n')  # Blue text\n",
    "\n",
    "for question in easy_questions:\n",
    "    API_RESPONSE = get_completion([{'role':'user','content':PROMPT.format(article=ada_lovelace_article,\n",
    "    question=question)}], model='gpt-4', logprobs=True)\n",
    "    print('\\033[92mQuestion:\\033[0m', question)  # Green text\n",
    "    for logprob in API_RESPONSE.choices[0].logprobs.content:\n",
    "        print(f\"\\033[96msufficient_context_for_answer:\\033[0m {logprob.token}, \\033[93mlogprobs:\\033[0m {logprob.logprob}, \\033[95mlinear probability:\\033[0m {np.round(np.exp(logprob.logprob)*100,2)}%\", '\\n')\n",
    "\n",
    "print('\\n\\n\\033[1mQuestions with potentially insufficient information\\033[0m\\n')  # Blue text\n",
    "\n",
    "for question in medium_questions:\n",
    "    API_RESPONSE = get_completion([{'role':'user','content':PROMPT.format(article=ada_lovelace_article,\n",
    "    question=question)}], model='gpt-4', logprobs=True,top_logprobs=3)\n",
    "    print('\\033[92mQuestion:\\033[0m', question)  # Green text\n",
    "    print(API_RESPONSE)\n",
    "    for logprob in API_RESPONSE.choices[0].logprobs.content:\n",
    "        print(f\"\\033[96msufficient_context_for_answer:\\033[0m {logprob.token}, \\033[93mlogprobs:\\033[0m {logprob.logprob}, \\033[95mlinear probability:\\033[0m {np.round(np.exp(logprob.logprob)*100,2)}%\", '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so we can see from the first two questions that our evaluator knows with (near) 100% confidence that the article has sufficient context to answer the posed question.\n",
    "On the other hand, for the more tricky question which are less clearly answered in the article, the model is signfiicantly less confident that it has sufficient context.\n",
    "This self-evaluation can help reduce hallucinations, as you can restrict answers, or ask for clearer questions, when your `sufficient_context_for_answer` log probability is below a certain threshold. Methods like this have been [shown](https://jfan001.medium.com/how-we-cut-the-rate-of-gpt-hallucinations-from-20-to-less-than-2-f3bfcc10e4ec) to significantly reduce RAG Q&A hallucinations and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Give me three random emojis\"\"\"\n",
    "API_RESPONSE = get_completion([{'role':'user','content':PROMPT}],model='gpt-3.5-turbo-1106',logprobs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-8XJGBUq8Q7VYU8dzwOjMV8WaUhOKE', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='\\\\xf0\\\\x9f', bytes=[240, 159], logprob=-0.17577635, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\x8c', bytes=[140], logprob=-0.85772306, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\x88', bytes=[136], logprob=-2.3395207, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\xf0\\\\x9f', bytes=[240, 159], logprob=-0.20028262, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\x8d', bytes=[141], logprob=-0.9647721, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\x95', bytes=[149], logprob=-0.5920826, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\xf0\\\\x9f', bytes=[240, 159], logprob=-0.15628104, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\x9a', bytes=[154], logprob=-0.67634493, top_logprobs=[]), ChatCompletionTokenLogprob(token='\\\\x80', bytes=[128], logprob=-0.0012997614, top_logprobs=[])]), message=ChatCompletionMessage(content='🌈🍕🚀', role='assistant', function_call=None, tool_calls=None))], created=1702949551, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_772e8125bb', usage=CompletionUsage(completion_tokens=9, prompt_tokens=12, total_tokens=21))"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "API_RESPONSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌈🍕🚀\n",
      "text = 🌈🍕🚀\n",
      "joint probability = 0.0025693992522193153\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "aggregated_bytes = []\n",
    "joint_logprob = 0.0\n",
    "for token in API_RESPONSE.choices[0].logprobs.content:\n",
    "    aggregated_bytes += token.bytes\n",
    "    joint_logprob += token.logprob\n",
    "\n",
    "message_content = API_RESPONSE.choices[0].message.content\n",
    "aggregated_text = bytes(aggregated_bytes).decode('utf-8')\n",
    "print(aggregated_text)\n",
    "\n",
    "assert message_content == aggregated_text\n",
    "\n",
    "print(f\"text = {aggregated_text}\")\n",
    "print(f\"joint probability = {exp(joint_logprob)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another use case for `logprobs` are autocomplete systems. Without creating the entire autocomplete engine end-to-end, let's demonstrate how `logprobs` could help us decide when we to suggest a sentence completion as a user is typing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's come up with a sample sentence: \"My least favorite TV show is Breaking Bad.\" Let's say we are building an autocomplete sentence, and we want it to dynamically recommend the next word or token as we are typing the sentence, but *only* if the model is quite sure of what the next word will be. To demonstrate this, let's break up the sentence into sequential components up to the title of the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = [\"My\",\"My least\", \"My least favorite\",\"My least favorite TV\",\"My least favorite TV show\",\n",
    "\"My least favorite TV show is\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: My\n",
      "\u001b[96mPredicted next token:\u001b[0m favorite, \u001b[93mlogprobs:\u001b[0m -0.18245785, \u001b[95mlinear probability:\u001b[0m 83.32%\n",
      "\u001b[96mPredicted next token:\u001b[0m dog, \u001b[93mlogprobs:\u001b[0m -2.397172, \u001b[95mlinear probability:\u001b[0m 9.1%\n",
      "\u001b[96mPredicted next token:\u001b[0m ap, \u001b[93mlogprobs:\u001b[0m -3.8732424, \u001b[95mlinear probability:\u001b[0m 2.08%\n",
      "\n",
      "\n",
      "Sentence: My least\n",
      "\u001b[96mPredicted next token:\u001b[0m favorite, \u001b[93mlogprobs:\u001b[0m -0.013642592, \u001b[95mlinear probability:\u001b[0m 98.65%\n",
      "\u001b[96mPredicted next token:\u001b[0m My, \u001b[93mlogprobs:\u001b[0m -4.3126197, \u001b[95mlinear probability:\u001b[0m 1.34%\n",
      "\u001b[96mPredicted next token:\u001b[0m  favorite, \u001b[93mlogprobs:\u001b[0m -9.684484, \u001b[95mlinear probability:\u001b[0m 0.01%\n",
      "\n",
      "\n",
      "Sentence: My least favorite\n",
      "\u001b[96mPredicted next token:\u001b[0m food, \u001b[93mlogprobs:\u001b[0m -0.9481721, \u001b[95mlinear probability:\u001b[0m 38.74%\n",
      "\u001b[96mPredicted next token:\u001b[0m My, \u001b[93mlogprobs:\u001b[0m -1.3447137, \u001b[95mlinear probability:\u001b[0m 26.06%\n",
      "\u001b[96mPredicted next token:\u001b[0m color, \u001b[93mlogprobs:\u001b[0m -1.3887696, \u001b[95mlinear probability:\u001b[0m 24.94%\n",
      "\n",
      "\n",
      "Sentence: My least favorite TV\n",
      "\u001b[96mPredicted next token:\u001b[0m show, \u001b[93mlogprobs:\u001b[0m -0.0007898556, \u001b[95mlinear probability:\u001b[0m 99.92%\n",
      "\u001b[96mPredicted next token:\u001b[0m My, \u001b[93mlogprobs:\u001b[0m -7.711523, \u001b[95mlinear probability:\u001b[0m 0.04%\n",
      "\u001b[96mPredicted next token:\u001b[0m series, \u001b[93mlogprobs:\u001b[0m -9.348547, \u001b[95mlinear probability:\u001b[0m 0.01%\n",
      "\n",
      "\n",
      "Sentence: My least favorite TV show\n",
      "\u001b[96mPredicted next token:\u001b[0m is, \u001b[93mlogprobs:\u001b[0m -0.18206136, \u001b[95mlinear probability:\u001b[0m 83.36%\n",
      "\u001b[96mPredicted next token:\u001b[0m of, \u001b[93mlogprobs:\u001b[0m -2.123914, \u001b[95mlinear probability:\u001b[0m 11.96%\n",
      "\u001b[96mPredicted next token:\u001b[0m My, \u001b[93mlogprobs:\u001b[0m -3.213298, \u001b[95mlinear probability:\u001b[0m 4.02%\n",
      "\n",
      "\n",
      "Sentence: My least favorite TV show is\n",
      "\u001b[96mPredicted next token:\u001b[0m \"My, \u001b[93mlogprobs:\u001b[0m -0.69349754, \u001b[95mlinear probability:\u001b[0m 49.98%\n",
      "\u001b[96mPredicted next token:\u001b[0m \"The, \u001b[93mlogprobs:\u001b[0m -1.2899293, \u001b[95mlinear probability:\u001b[0m 27.53%\n",
      "\u001b[96mPredicted next token:\u001b[0m My, \u001b[93mlogprobs:\u001b[0m -2.4170141, \u001b[95mlinear probability:\u001b[0m 8.92%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentence_list:\n",
    "  PROMPT = \"\"\"Complete this sentence. You are acting as auto-complete. Simply complete the sentence to the best of your ability, make sure it is just ONE sentence: {sentence}\"\"\"\n",
    "  API_RESPONSE = get_completion([{'role':'user','content':PROMPT.format(sentence=sentence)}],model='gpt-3.5-turbo',logprobs=True,top_logprobs=3)\n",
    "#  for next_token in API_RESPONSE.choices[0].logprobs.content[0]:\n",
    "  print('Sentence:',sentence)\n",
    "\n",
    "  for alt_token in API_RESPONSE.choices[0].logprobs.content[0].top_logprobs:\n",
    "    print(f\"\\033[96mPredicted next token:\\033[0m {alt_token.token}, \\033[93mlogprobs:\\033[0m {alt_token.logprob}, \\033[95mlinear probability:\\033[0m {np.round(np.exp(alt_token.logprob)*100,2)}%\")\n",
    "    if np.exp(alt_token.logprob)>.95:\n",
    "      high_prob_completions[sentence] = alt_token.token\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! If we were to create an autocomplete system using `gpt-3.5-turbo`, we could set the threshold to recommend a completion at whatever probability we want, say 95% linear probability. This would have our autocompletion engine recommend \"favorite\" after we say \"My least\" (which is reasonable), but not have any recommendation after \"My least favorite TV show is\" (which makes sense as we don't want our autocomplete guessing our favorite show!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other use cases for `logprobs` that are not covered in this notebook. We can use `logprobs` to calculate the `perplexity` of your outputs (the evaluation metric of uncertainty or surprise of the model at its outcomes). This can be calculated by using `logprobs` to calculate the exponentatied average negative log-likelihood of all of our output tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"My\n",
      "\"My least\n",
      "\"My least favorite\n",
      "\"My least favorite TV\n",
      "\"My least favorite TV show\n",
      "\"My least favorite TV show is\n",
      "\"My least favorite TV show is any\n",
      "\"My least favorite TV show is any reality\n",
      "\"My least favorite TV show is any reality show\n",
      "\"My least favorite TV show is any reality show that\n",
      "\"My least favorite TV show is any reality show that focuses\n",
      "\"My least favorite TV show is any reality show that focuses on\n",
      "\"My least favorite TV show is any reality show that focuses on drama\n",
      "\"My least favorite TV show is any reality show that focuses on drama and\n",
      "\"My least favorite TV show is any reality show that focuses on drama and gossip\n",
      "\"My least favorite TV show is any reality show that focuses on drama and gossip.\"\n",
      "text = \"My least favorite TV show is any reality show that focuses on drama and gossip.\"\n",
      "joint probability = 1.1513921327020997e-07\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "aggregated_bytes = []\n",
    "joint_logprob = 0.0\n",
    "for token in API_RESPONSE.choices[0].logprobs.content:\n",
    "    aggregated_bytes += token.bytes\n",
    "    # Add the logprob of the current token to the joint logprob\n",
    "    joint_logprob += token.logprob\n",
    "\n",
    "    # Get the content of the message from the API response\n",
    "    message_content = API_RESPONSE.choices[0].message.content\n",
    "\n",
    "    # Decode the aggregated bytes to text\n",
    "    aggregated_text = bytes(aggregated_bytes).decode('utf-8')\n",
    "\n",
    "    # Print the aggregated text\n",
    "    print(aggregated_text)\n",
    "\n",
    "assert message_content == aggregated_text\n",
    "\n",
    "print(f\"text = {aggregated_text}\")\n",
    "print(f\"joint probability = {exp(joint_logprob)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence\n",
    "PROMPT = \"\"\"Complete this sentence: {sentence}\"\"\"\n",
    "API_RESPONSE = get_completion([{'role':'user','content':PROMPT.format(sentence=sentence)}],model='gpt-3.5-turbo',logprobs=True,top_logprobs=5)\n",
    "\n",
    "#Function to highlight each token\n",
    "def highlight_text(api_response):\n",
    "    colors = ['\\033[95m', '\\033[92m', '\\033[93m', '\\033[91m', '\\033[94m']  # ANSI codes for purple, green, orange, red, blue\n",
    "    reset_color = '\\033[0m'\n",
    "    tokens = api_response.choices[0].logprobs.content\n",
    "\n",
    "    color_idx = 0\n",
    "    for t in tokens:\n",
    "        token_str = bytes(t.bytes).decode('utf-8')\n",
    "        print(f\"{colors[color_idx]}{token_str}{reset_color}\", end=\"\")\n",
    "\n",
    "        # Move to the next color in the sequence, wrapping around if necessary\n",
    "        color_idx = (color_idx + 1) % len(colors)\n",
    "    print()  # for readability\n",
    "    print(f\"Total number of tokens: {len(tokens)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
